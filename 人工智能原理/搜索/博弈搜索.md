# 博弈搜索

# 一、Minimax算法

![minimax](pngs/minimax.png)
> 已知MIN会选择最小值  
> MAX怎么选才能拿到最大值？

- 遍历搜索，复杂度为$O(b^m)$
    - $b$为分支数，$m$为树的深度

# 二、$\alpha-\beta$ 剪枝

## 2.1 剪枝示意图

![alpha_beta](pngs/alpha_beta.png)

#### 更多知识

1. 通过维护$(\alpha, \beta)$，来剪枝
2. 剪枝的效率很大程度上，取决于**后继的顺序**
    - 例如上图中，最后一个MIN节点。如果子节点顺序为[2,5,14]，也可以剪枝。

## 2.2 引入 启发式

> 即使$\alpha-\beta$剪枝，也搜索不完，怎么办？

<details>
<summary>两种策略</summary>

![strategies](pngs/strategies.png)

</details>

### 2.2.1 策略A：限制树的深度

例如我只搜索到第7层。  
1. 如果棋局已经结束，当然可以知道真实得分
2. 如果棋局未结束，用一个**评估函数**，来估算得分。当作最终对局结束的分值

### 2.2.2 策略B：只考虑少数分支

例如我们下象棋，开局几步总是优先考虑`车`、`马`、`炮`，没必要考虑所有情况。

# 三、蒙特卡洛树搜索

> ```mermaid
> graph LR
>     MCTS & UCB1 --- UCT
> ```

## 3.1 MCTS

![MCTS](pngs/MCTS.png)

## 3.2 上置信界算法UCB1

> 随机模拟，效率较低  
> 需要引导博弈树去探索更有可能取胜的分支  
> 于是介绍了一下**UCB1**  
> ~~作业里要自己实现的，这里就不详细记了~~

## 3.3 UCT

![UCT](pngs/UCT.png)

## 3.4 详细流程

<details>
<summary>1. 选择</summary>

![MCTS_select](pngs/MCTS_select.png)

</details>

<details>
<summary>2. 扩展</summary>

![MCTS_expand](pngs/MCTS_expand.png)

</details>

<details>
<summary>3. 模拟</summary>

![MCTS_simulate](pngs/MCTS_simulate.png)

</details>

<details>
<summary>4. 更新</summary>

![MCTS_BP](pngs/MCTS_BP.png)

</details>

---

# 四、AlphaGo

设计`policy网络`和`value网络`改进 **MCTS**

![policy&value](pngs/policy&value.png)

> 1. policy网络
>    - 对应**策略B**，鼓励我们去探索少数分支
> 2. value网络
>    - 对应**策略A**，探索到一定深度，我们评估一下最终得分，直接返回

## 4.1 详细流程

我们来具体看看**AlphaGo**中，这4步是怎么做的。