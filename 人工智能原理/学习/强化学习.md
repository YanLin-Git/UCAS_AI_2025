# 强化学习

> 下学期还有专门的**强化学习**课程，慢慢来

# 一、基本概念

## 1.1 MDP五元组

![MDP](pngs/MDP.png)

## 1.2 强化学习的目标

![RL_object](pngs/RL_object.png)

## 1.3 Q函数、V函数

![Q&V](pngs/Q&V.png)
> **Q(s,a)**: 处于状态s时，采取动作a，能够获得的期望奖励。  
> **V(s)**: 处于状态s时，能够获得的期望奖励

## 1.4 贝尔曼方程

#### 1) 从这张图来看，更直观：

![Bellman_equations](pngs/Bellman_equations.png)
> 上图中使用的奖励值为：$R_s^a=E[r_t|s_t=s, a_t=a]$  
> 与课件中的$R_{ss'}^a$，所代表的含义有差异。

#### 2) 贝尔曼最优方程

![Bellman_optimality_equations](pngs/Bellman_optimality_equations.png)

## 1.5 几种强化学习算法

![RL_algorithms](pngs/RL_algorithms.png)

# 二、优化算法

## 2.1 策略参数化

> $\pi_\theta(a|s)$: 处于状态s时，采取动作a的概率。  
> 怎么计算呢？

对于**离散动作空间**，常用Softmax策略:
$$
\pi_\theta(a|s) = \frac {e^{f_\theta (s,a)}} {\sum\limits_{a'} e^{f_\theta (s,a')}}
$$

> 例如最熟悉的语言模型  
> 这里$f_\theta (s,a)$，就是指神经网络计算出的`logits`  
> 那么生成每个token的概率即为`Softmax(logits)`

## 2.2 策略梯度算法

![PG](pngs/PG.png)

## 2.3 Actor-Critic架构

### 2.3.1 AC（Actor-Critic）

#### 1) 使用$Q_{\phi}(s,a)$来代替$r_{sa}$

![AC_1](pngs/AC_1.png)

#### 2) 同时训练两个网络$Q_\phi(s,a)、\pi_\theta(a|s)$

![AC_2](pngs/AC_2.png)

### 2.3.2 A2C（Advantageous Actor-Critic）

更进一步，添加了一个baseline  
使用优势函数$A(s,a)$来代替**AC**中的$Q(s,a)$

![A2C](pngs/A2C.png)

### 2.3.3 A3C（Asynchronous Advantage Actor Critic）

> 这部分没看懂，略

## 2.4 TRPO

- 选择介绍了**TRPO**中的两个技术：
    1. 重要性采样
    2. KL散度约束

## 2.5 PPO

1. **TRPO**使用`KL散度约束`，来限制**策略更新的幅度**
    - **PPO**则直接`对目标函数做截断`，来近似`KL散度约束`
2. 使用`GAE`来估算Advantages

> `作业4`给了一份示例代码，非常值得精读。  
> 专门总结一篇：[PPO](人工智能原理/学习/PPO.md)

## 2.6 GRPO

> 学习完**PPO**，两张图就可以解释清楚**GRPO**

<details>
<summary>1. 计算Advantage的方式</summary>

![GRPO_1](pngs/GRPO_1.png)
> 1. **PPO**理论更完备，使用`GAE`来估算。
>    - 需要准备好$r、v$
>    - 为了得到$v$，需额外引入`value model`
> 2. **GRPO**通过一些简化操作来近似。
>    - 准备一批$r_1, r_2, \dots, r_G$
>    - 直接这样近似计算：
>    $$
>    A_i = r_i - \frac {r_1 + r_2 + \dots + r_G} G
>    $$

</details>

<details>
<summary>2. 训练架构</summary>

![GRPO_2](pngs/GRPO_2.png)
> 1. **PPO**中，需训练两个模型`policy model`、`value model`
>    - 使用`actor-critic架构`来训练
> 2. **GRPO**简化之后，只需训练`policy model`
>    - 又回到了单纯的`策略梯度算法`

</details>